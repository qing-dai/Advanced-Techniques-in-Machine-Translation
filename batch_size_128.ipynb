{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dIdAktXW6X5",
        "outputId": "e277676a-2124-4806-a68d-b5d075c72f34"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghn0EX0Nbi9u",
        "outputId": "c9d359ef-e95f-4478-bb89-a1301babea68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cloning into '/content/drive/My Drive/atmt_2023_batch_s'...\n",
            "remote: Enumerating objects: 271, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 271 (delta 0), reused 2 (delta 0), pack-reused 267\u001b[K\n",
            "Receiving objects: 100% (271/271), 29.38 MiB | 14.68 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n",
            "Updating files: 100% (157/157), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone -b hyper-parameter --single-branch https://qing-dai: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@github.com/qing-dai/atmt_2023.git '/content/drive/My Drive/atmt_2023_batch_s'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2UfEY7ZLKCD",
        "outputId": "90c80c9e-19a3-493c-841d-c8d2360a4048"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/118.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/atmt_2023_batch_s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq5bC9UFcT3K",
        "outputId": "3603d2bd-3e79-41ce-f897-922b5d6b0be5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/atmt_2023_batch_s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py \\\n",
        "  --data /content/drive/MyDrive/atmt_2023_batch_s//data/en-fr/prepared \\\n",
        "  --save-dir /content/drive/MyDrive/atmt_2023_batch_s//assignments/03/baseline/checkpoints \\\n",
        "  --cuda \\\n",
        "  --batch-size 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VcdxUVhv17e",
        "outputId": "a9d3b04a-046d-466a-dc23-6ab92e87a6b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Commencing training!\n",
            "INFO: COMMAND: train.py --data /content/drive/MyDrive/atmt_2023_batch_s//data/en-fr/prepared --save-dir /content/drive/MyDrive/atmt_2023_batch_s//assignments/03/baseline/checkpoints --cuda --batch-size 128\n",
            "INFO: Arguments: {'cuda': True, 'data': '/content/drive/MyDrive/atmt_2023_batch_s//data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/content/drive/MyDrive/atmt_2023_batch_s//assignments/03/baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
            "INFO: Loaded a source dictionary (fr) with 4000 words\n",
            "INFO: Loaded a target dictionary (en) with 4000 words\n",
            "INFO: Built a model with 1308576 parameters\n",
            "INFO: Loaded checkpoint /content/drive/MyDrive/atmt_2023_batch_s//assignments/03/baseline/checkpoints/checkpoint_last.pt\n",
            "INFO: Epoch 056: loss 1.893 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 8.484 | clip 0.9873\n",
            "INFO: Epoch 056: valid_loss 2.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.82\n",
            "INFO: Epoch 057: loss 1.781 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 6.236 | clip 0.6582\n",
            "INFO: Epoch 057: valid_loss 2.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.64\n",
            "INFO: Epoch 058: loss 1.748 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 7.173 | clip 0.6456\n",
            "INFO: Epoch 058: valid_loss 2.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.54\n",
            "INFO: Epoch 059: loss 1.72 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 6.723 | clip 0.6329\n",
            "INFO: Epoch 059: valid_loss 2.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.45\n",
            "INFO: Epoch 060: loss 1.701 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.887 | clip 0.6456\n",
            "INFO: Epoch 060: valid_loss 2.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.39\n",
            "INFO: Epoch 061: loss 1.68 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 6.295 | clip 0.6203\n",
            "INFO: Epoch 061: valid_loss 2.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.34\n",
            "INFO: Epoch 062: loss 1.663 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.694 | clip 0.6329\n",
            "INFO: Epoch 062: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.29\n",
            "INFO: Epoch 063: loss 1.641 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.783 | clip 0.5949\n",
            "INFO: Epoch 063: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.26\n",
            "INFO: Epoch 064: loss 1.628 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 6.185 | clip 0.6076\n",
            "INFO: Epoch 064: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.24\n",
            "INFO: Epoch 065: loss 1.613 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.651 | clip 0.6076\n",
            "INFO: Epoch 065: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.18\n",
            "INFO: Epoch 066: loss 1.595 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.551 | clip 0.5949\n",
            "INFO: Epoch 066: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.15\n",
            "INFO: Epoch 067: loss 1.578 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.784 | clip 0.5696\n",
            "INFO: Epoch 067: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.15\n",
            "INFO: Epoch 068: loss 1.565 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.63 | clip 0.5696\n",
            "INFO: Epoch 068: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.1\n",
            "INFO: Epoch 069: loss 1.556 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.805 | clip 0.5823\n",
            "INFO: Epoch 069: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.08\n",
            "INFO: Epoch 070: loss 1.542 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.553 | clip 0.6076\n",
            "INFO: Epoch 070: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.06\n",
            "INFO: Epoch 071: loss 1.527 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.63 | clip 0.6329\n",
            "INFO: Epoch 071: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.03\n",
            "INFO: Epoch 072: loss 1.512 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.522 | clip 0.5949\n",
            "INFO: Epoch 072: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.01\n",
            "INFO: Epoch 073: loss 1.503 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.626 | clip 0.6076\n",
            "INFO: Epoch 073: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8\n",
            "INFO: Epoch 074: loss 1.493 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.312 | clip 0.5696\n",
            "INFO: Epoch 074: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.96\n",
            "INFO: Epoch 075: loss 1.482 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 6.057 | clip 0.5823\n",
            "INFO: Epoch 075: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.96\n",
            "INFO: Epoch 076: loss 1.466 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.417 | clip 0.6203\n",
            "INFO: Epoch 076: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.93\n",
            "INFO: Epoch 077: loss 1.463 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.481 | clip 0.5949\n",
            "INFO: Epoch 077: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.9\n",
            "INFO: Epoch 078: loss 1.45 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.441 | clip 0.6203\n",
            "INFO: Epoch 078: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.91\n",
            "INFO: Epoch 079: loss 1.441 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.648 | clip 0.5823\n",
            "INFO: Epoch 079: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.88\n",
            "INFO: Epoch 080: loss 1.433 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.613 | clip 0.5949\n",
            "INFO: Epoch 080: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.87\n",
            "INFO: Epoch 081: loss 1.419 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.943 | clip 0.5949\n",
            "INFO: Epoch 081: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.85\n",
            "INFO: Epoch 082: loss 1.415 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.402 | clip 0.5949\n",
            "INFO: Epoch 082: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.85\n",
            "INFO: Epoch 083: loss 1.404 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 6.025 | clip 0.6329\n",
            "INFO: Epoch 083: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.83\n",
            "INFO: Epoch 084: loss 1.395 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.544 | clip 0.5823\n",
            "INFO: Epoch 084: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.84\n",
            "INFO: Epoch 085: loss 1.387 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.516 | clip 0.5696\n",
            "INFO: Epoch 085: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.8\n",
            "INFO: Epoch 086: loss 1.373 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.465 | clip 0.5696\n",
            "INFO: Epoch 086: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.79\n",
            "INFO: Epoch 087: loss 1.366 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.648 | clip 0.6203\n",
            "INFO: Epoch 087: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.79\n",
            "INFO: Epoch 088: loss 1.358 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.903 | clip 0.5823\n",
            "INFO: Epoch 088: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.79\n",
            "INFO: Epoch 089: loss 1.35 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.471 | clip 0.5949\n",
            "INFO: Epoch 089: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.77\n",
            "INFO: Epoch 090: loss 1.34 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.433 | clip 0.5696\n",
            "INFO: Epoch 090: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.75\n",
            "INFO: Epoch 091: loss 1.337 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.442 | clip 0.6076\n",
            "INFO: Epoch 091: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.77\n",
            "INFO: Epoch 092: loss 1.324 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.575 | clip 0.5823\n",
            "INFO: Epoch 092: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.75\n",
            "INFO: Epoch 093: loss 1.322 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.579 | clip 0.5823\n",
            "INFO: Epoch 093: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.76\n",
            "INFO: Epoch 094: loss 1.308 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.525 | clip 0.6076\n",
            "INFO: Epoch 094: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.77\n",
            "INFO: Epoch 095: loss 1.302 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.403 | clip 0.5696\n",
            "INFO: Epoch 095: valid_loss 2.05 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.73\n",
            "INFO: Epoch 096: loss 1.294 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.347 | clip 0.5443\n",
            "INFO: Epoch 096: valid_loss 2.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.72\n",
            "INFO: Epoch 097: loss 1.291 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.392 | clip 0.5823\n",
            "INFO: Epoch 097: valid_loss 2.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.7\n",
            "INFO: Epoch 098: loss 1.284 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.841 | clip 0.5949\n",
            "INFO: Epoch 098: valid_loss 2.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.69\n",
            "INFO: Epoch 099: loss 1.275 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.655 | clip 0.5949\n",
            "INFO: Epoch 099: valid_loss 2.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.7\n",
            "INFO: Epoch 100: loss 1.27 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.476 | clip 0.5823\n",
            "INFO: Epoch 100: valid_loss 2.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.72\n",
            "INFO: Epoch 101: loss 1.265 | lr 0.0003 | num_tokens 9.586 | batch_size 126.6 | grad_norm 5.35 | clip 0.5696\n",
            "INFO: Epoch 101: valid_loss 2.04 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.71\n",
            "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run from here on GPU"
      ],
      "metadata": {
        "id": "w1v7oZeWDBH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate our test set from French to English"
      ],
      "metadata": {
        "id": "acTJRczjEOWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 translate.py \\\n",
        "    --data /content/drive/MyDrive/atmt_2023_batch_s/data/en-fr/prepared \\\n",
        "    --dicts /content/drive/MyDrive/atmt_2023_batch_s/data/en-fr/prepared \\\n",
        "    --checkpoint-path /content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/checkpoints/checkpoint_last.pt \\\n",
        "    --output /content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/translations.p2.txt \\\n",
        "    --batch-size 128 \\\n",
        "    --cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoCPauKHigp2",
        "outputId": "bf54f334-ba26-46a5-8a4b-0ae3a436c578"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-11-06 16:03:58] COMMAND: translate.py --data /content/drive/MyDrive/atmt_2023_batch_s/data/en-fr/prepared --dicts /content/drive/MyDrive/atmt_2023_batch_s/data/en-fr/prepared --checkpoint-path /content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/checkpoints/checkpoint_last.pt --output /content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/translations.p2.txt --batch-size 128 --cuda\n",
            "[2023-11-06 16:03:58] Arguments: {'cuda': True, 'data': '/content/drive/MyDrive/atmt_2023_batch_s/data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/content/drive/MyDrive/atmt_2023_batch_s//assignments/03/baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/content/drive/MyDrive/atmt_2023_batch_s/data/en-fr/prepared', 'checkpoint_path': '/content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/checkpoints/checkpoint_last.pt', 'output': '/content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/translations.p2.txt', 'max_len': 128}\n",
            "[2023-11-06 16:03:58] Loaded a source dictionary (fr) with 4000 words\n",
            "[2023-11-06 16:03:58] Loaded a target dictionary (en) with 4000 words\n",
            "[2023-11-06 16:03:59] Loaded a model from checkpoint /content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/checkpoints/checkpoint_last.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To postprocess the generated translations and produce raw text, use the postprocess.sh located in the scripts directory"
      ],
      "metadata": {
        "id": "SZepKvydEY0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/drive/MyDrive/atmt_2023_batch_s/scripts/postprocess.sh \\\n",
        "    /content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/translations.p2.txt \\\n",
        "    /content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/translations.p2.p.txt en"
      ],
      "metadata": {
        "id": "6HIZcoP0DFsy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate our NMT system, execute the following command:"
      ],
      "metadata": {
        "id": "0axKlIl1EnCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \\\n",
        "    /content/drive/MyDrive/atmt_2023_batch_s/assignments/03/baseline/translations.p2.p.txt \\\n",
        "    | sacrebleu /content/drive/MyDrive/atmt_2023_batch_s/data/en-fr/raw/test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqfsJTppE0Qr",
        "outputId": "9973bd2f-40a9-4bae-fa2d-cf572d459550"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 27.1,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
            " \"verbose_score\": \"59.9/33.4/20.8/13.0 (BP = 0.998 ratio = 0.998 hyp_len = 3885 ref_len = 3892)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.3.1\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmLMZbf0UpvM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}