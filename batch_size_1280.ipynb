{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghn0EX0Nbi9u",
    "outputId": "e5b8d9e7-add9-4278-98e2-9aa05b2bee6f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "Cloning into '/content/drive/My Drive/atmt_2023_batch'...\n",
      "remote: Enumerating objects: 271, done.\u001B[K\n",
      "remote: Counting objects: 100% (4/4), done.\u001B[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001B[K\n",
      "remote: Total 271 (delta 0), reused 2 (delta 0), pack-reused 267\u001B[K\n",
      "Receiving objects: 100% (271/271), 29.38 MiB | 16.39 MiB/s, done.\n",
      "Resolving deltas: 100% (64/64), done.\n",
      "Updating files: 100% (157/157), done.\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!git clone -b hyper-parameter --single-branch https://qing-dai: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@github.com/qing-dai/atmt_2023.git '/content/drive/My Drive/atmt_2023_batch'\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5tTE5wmSfEI",
    "outputId": "a185e032-1c62-45aa-fbe2-4bb0d6e735a3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install sacrebleu"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2UfEY7ZLKCD",
    "outputId": "06f46a99-c490-46bf-a98a-7aa9552bc8bd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/119.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m112.6/119.7 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m119.7/119.7 kB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
      "Installing collected packages: portalocker, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.3.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "cd /content/drive/MyDrive/atmt_2023_batch"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mq5bC9UFcT3K",
    "outputId": "6d2104e2-e09f-441d-d70b-c51f19ce870f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/MyDrive/atmt_2023_batch\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python3 train.py \\\n",
    "  --data /content/drive/MyDrive/atmt_2023_batch/data/en-fr/prepared \\\n",
    "  --save-dir /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/checkpoints \\\n",
    "  --cuda \\\n",
    "  --batch-size 1280"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VcdxUVhv17e",
    "outputId": "73f823f6-35a8-42bb-f02e-7d5f687f6bb8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: Commencing training!\n",
      "INFO: COMMAND: train.py --data /content/drive/MyDrive/atmt_2023_batch/data/en-fr/prepared --save-dir /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/checkpoints --cuda --batch-size 1280\n",
      "INFO: Arguments: {'cuda': True, 'data': '/content/drive/MyDrive/atmt_2023_batch/data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1280, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}\n",
      "INFO: Loaded a source dictionary (fr) with 4000 words\n",
      "INFO: Loaded a target dictionary (en) with 4000 words\n",
      "INFO: Built a model with 1308576 parameters\n",
      "INFO: Loaded checkpoint /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/checkpoints/checkpoint_last.pt\n",
      "INFO: Epoch 056: loss 2.129 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 6.606 | clip 1\n",
      "INFO: Epoch 056: valid_loss 2.34 | num_tokens 9.14 | batch_size 500 | valid_perplexity 10.4\n",
      "INFO: Epoch 057: loss 1.957 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 3.965 | clip 0.25\n",
      "INFO: Epoch 057: valid_loss 2.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.16\n",
      "INFO: Epoch 058: loss 1.862 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 3.184 | clip 0.125\n",
      "INFO: Epoch 058: valid_loss 2.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.06\n",
      "INFO: Epoch 059: loss 1.834 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 3.265 | clip 0.125\n",
      "INFO: Epoch 059: valid_loss 2.21 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.14\n",
      "INFO: Epoch 060: loss 1.812 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 2.864 | clip 0.125\n",
      "INFO: Epoch 060: valid_loss 2.2 | num_tokens 9.14 | batch_size 500 | valid_perplexity 9.01\n",
      "INFO: Epoch 061: loss 1.793 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 2.232 | clip 0.125\n",
      "INFO: Epoch 061: valid_loss 2.18 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.86\n",
      "INFO: Epoch 062: loss 1.785 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 2.051 | clip 0.125\n",
      "INFO: Epoch 062: valid_loss 2.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.78\n",
      "INFO: Epoch 063: loss 1.783 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.999 | clip 0.125\n",
      "INFO: Epoch 063: valid_loss 2.17 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.73\n",
      "INFO: Epoch 064: loss 1.773 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 2.015 | clip 0.125\n",
      "INFO: Epoch 064: valid_loss 2.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.7\n",
      "INFO: Epoch 065: loss 1.76 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.963 | clip 0.125\n",
      "INFO: Epoch 065: valid_loss 2.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.67\n",
      "INFO: Epoch 066: loss 1.757 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.996 | clip 0.125\n",
      "INFO: Epoch 066: valid_loss 2.16 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.65\n",
      "INFO: Epoch 067: loss 1.749 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.925 | clip 0.125\n",
      "INFO: Epoch 067: valid_loss 2.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.63\n",
      "INFO: Epoch 068: loss 1.744 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.85 | clip 0.125\n",
      "INFO: Epoch 068: valid_loss 2.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.6\n",
      "INFO: Epoch 069: loss 1.75 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.874 | clip 0\n",
      "INFO: Epoch 069: valid_loss 2.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.58\n",
      "INFO: Epoch 070: loss 1.737 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.884 | clip 0\n",
      "INFO: Epoch 070: valid_loss 2.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.57\n",
      "INFO: Epoch 071: loss 1.731 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.93 | clip 0.125\n",
      "INFO: Epoch 071: valid_loss 2.15 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.55\n",
      "INFO: Epoch 072: loss 1.725 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.877 | clip 0.125\n",
      "INFO: Epoch 072: valid_loss 2.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.53\n",
      "INFO: Epoch 073: loss 1.719 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.805 | clip 0\n",
      "INFO: Epoch 073: valid_loss 2.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.51\n",
      "INFO: Epoch 074: loss 1.711 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.805 | clip 0\n",
      "INFO: Epoch 074: valid_loss 2.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.5\n",
      "INFO: Epoch 075: loss 1.708 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.822 | clip 0\n",
      "INFO: Epoch 075: valid_loss 2.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.48\n",
      "INFO: Epoch 076: loss 1.701 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.848 | clip 0.125\n",
      "INFO: Epoch 076: valid_loss 2.14 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.46\n",
      "INFO: Epoch 077: loss 1.695 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.801 | clip 0\n",
      "INFO: Epoch 077: valid_loss 2.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.44\n",
      "INFO: Epoch 078: loss 1.695 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.779 | clip 0\n",
      "INFO: Epoch 078: valid_loss 2.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.43\n",
      "INFO: Epoch 079: loss 1.69 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.855 | clip 0.125\n",
      "INFO: Epoch 079: valid_loss 2.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.42\n",
      "INFO: Epoch 080: loss 1.687 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.776 | clip 0\n",
      "INFO: Epoch 080: valid_loss 2.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.4\n",
      "INFO: Epoch 081: loss 1.687 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.781 | clip 0\n",
      "INFO: Epoch 081: valid_loss 2.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.38\n",
      "INFO: Epoch 082: loss 1.678 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.786 | clip 0\n",
      "INFO: Epoch 082: valid_loss 2.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.37\n",
      "INFO: Epoch 083: loss 1.672 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.79 | clip 0\n",
      "INFO: Epoch 083: valid_loss 2.13 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.38\n",
      "INFO: Epoch 084: loss 1.664 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.798 | clip 0\n",
      "INFO: Epoch 084: valid_loss 2.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.37\n",
      "INFO: Epoch 085: loss 1.664 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.784 | clip 0\n",
      "INFO: Epoch 085: valid_loss 2.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.35\n",
      "INFO: Epoch 086: loss 1.659 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.735 | clip 0\n",
      "INFO: Epoch 086: valid_loss 2.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.34\n",
      "INFO: Epoch 087: loss 1.656 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.742 | clip 0\n",
      "INFO: Epoch 087: valid_loss 2.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.33\n",
      "INFO: Epoch 088: loss 1.653 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.77 | clip 0\n",
      "INFO: Epoch 088: valid_loss 2.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.31\n",
      "INFO: Epoch 089: loss 1.646 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.725 | clip 0\n",
      "INFO: Epoch 089: valid_loss 2.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.3\n",
      "INFO: Epoch 090: loss 1.64 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.718 | clip 0\n",
      "INFO: Epoch 090: valid_loss 2.12 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.29\n",
      "INFO: Epoch 091: loss 1.639 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.74 | clip 0\n",
      "INFO: Epoch 091: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.29\n",
      "INFO: Epoch 092: loss 1.635 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.735 | clip 0\n",
      "INFO: Epoch 092: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.28\n",
      "INFO: Epoch 093: loss 1.632 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.802 | clip 0.125\n",
      "INFO: Epoch 093: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.27\n",
      "INFO: Epoch 094: loss 1.629 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.717 | clip 0\n",
      "INFO: Epoch 094: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.25\n",
      "INFO: Epoch 095: loss 1.623 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.735 | clip 0\n",
      "INFO: Epoch 095: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.25\n",
      "INFO: Epoch 096: loss 1.619 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.72 | clip 0\n",
      "INFO: Epoch 096: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.24\n",
      "INFO: Epoch 097: loss 1.613 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.729 | clip 0\n",
      "INFO: Epoch 097: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.24\n",
      "INFO: Epoch 098: loss 1.613 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.707 | clip 0\n",
      "INFO: Epoch 098: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.23\n",
      "INFO: Epoch 099: loss 1.61 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.691 | clip 0\n",
      "INFO: Epoch 099: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.21\n",
      "INFO: Epoch 100: loss 1.609 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.708 | clip 0\n",
      "INFO: Epoch 100: valid_loss 2.11 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.21\n",
      "INFO: Epoch 101: loss 1.601 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.719 | clip 0\n",
      "INFO: Epoch 101: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.21\n",
      "INFO: Epoch 102: loss 1.6 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.782 | clip 0.125\n",
      "INFO: Epoch 102: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.19\n",
      "INFO: Epoch 103: loss 1.597 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.703 | clip 0\n",
      "INFO: Epoch 103: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.18\n",
      "INFO: Epoch 104: loss 1.597 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.734 | clip 0\n",
      "INFO: Epoch 104: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.17\n",
      "INFO: Epoch 105: loss 1.593 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.705 | clip 0\n",
      "INFO: Epoch 105: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.17\n",
      "INFO: Epoch 106: loss 1.588 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.671 | clip 0\n",
      "INFO: Epoch 106: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.16\n",
      "INFO: Epoch 107: loss 1.586 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.708 | clip 0\n",
      "INFO: Epoch 107: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.15\n",
      "INFO: Epoch 108: loss 1.578 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.673 | clip 0\n",
      "INFO: Epoch 108: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.15\n",
      "INFO: Epoch 109: loss 1.578 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.673 | clip 0\n",
      "INFO: Epoch 109: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.14\n",
      "INFO: Epoch 110: loss 1.573 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.667 | clip 0\n",
      "INFO: Epoch 110: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.14\n",
      "INFO: Epoch 111: loss 1.573 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.74 | clip 0\n",
      "INFO: Epoch 111: valid_loss 2.1 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.13\n",
      "INFO: Epoch 112: loss 1.567 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.715 | clip 0\n",
      "INFO: Epoch 112: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.12\n",
      "INFO: Epoch 113: loss 1.565 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.705 | clip 0\n",
      "INFO: Epoch 113: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.11\n",
      "INFO: Epoch 114: loss 1.559 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.672 | clip 0\n",
      "INFO: Epoch 114: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.1\n",
      "INFO: Epoch 115: loss 1.567 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.721 | clip 0\n",
      "INFO: Epoch 115: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.09\n",
      "INFO: Epoch 116: loss 1.56 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.695 | clip 0\n",
      "INFO: Epoch 116: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.09\n",
      "INFO: Epoch 117: loss 1.553 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.715 | clip 0\n",
      "INFO: Epoch 117: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.09\n",
      "INFO: Epoch 118: loss 1.549 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.739 | clip 0\n",
      "INFO: Epoch 118: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.09\n",
      "INFO: Epoch 119: loss 1.552 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.679 | clip 0\n",
      "INFO: Epoch 119: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.08\n",
      "INFO: Epoch 120: loss 1.544 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.672 | clip 0\n",
      "INFO: Epoch 120: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.08\n",
      "INFO: Epoch 121: loss 1.543 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.681 | clip 0\n",
      "INFO: Epoch 121: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.08\n",
      "INFO: Epoch 122: loss 1.543 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.707 | clip 0\n",
      "INFO: Epoch 122: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.07\n",
      "INFO: Epoch 123: loss 1.54 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.672 | clip 0\n",
      "INFO: Epoch 123: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.05\n",
      "INFO: Epoch 124: loss 1.527 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.654 | clip 0\n",
      "INFO: Epoch 124: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.05\n",
      "INFO: Epoch 125: loss 1.53 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.714 | clip 0\n",
      "INFO: Epoch 125: valid_loss 2.09 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.05\n",
      "INFO: Epoch 126: loss 1.532 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.691 | clip 0\n",
      "INFO: Epoch 126: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.04\n",
      "INFO: Epoch 127: loss 1.528 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.67 | clip 0\n",
      "INFO: Epoch 127: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.03\n",
      "INFO: Epoch 128: loss 1.528 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.679 | clip 0\n",
      "INFO: Epoch 128: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.03\n",
      "INFO: Epoch 129: loss 1.525 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.653 | clip 0\n",
      "INFO: Epoch 129: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.03\n",
      "INFO: Epoch 130: loss 1.519 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.736 | clip 0\n",
      "INFO: Epoch 130: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8.01\n",
      "INFO: Epoch 131: loss 1.51 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.667 | clip 0\n",
      "INFO: Epoch 131: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8\n",
      "INFO: Epoch 132: loss 1.509 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.655 | clip 0\n",
      "INFO: Epoch 132: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8\n",
      "INFO: Epoch 133: loss 1.513 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.706 | clip 0\n",
      "INFO: Epoch 133: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 8\n",
      "INFO: Epoch 134: loss 1.51 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.668 | clip 0\n",
      "INFO: Epoch 134: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.99\n",
      "INFO: Epoch 135: loss 1.51 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.67 | clip 0\n",
      "INFO: Epoch 135: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.99\n",
      "INFO: Epoch 136: loss 1.501 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.666 | clip 0\n",
      "INFO: Epoch 136: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.98\n",
      "INFO: Epoch 137: loss 1.499 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.62 | clip 0\n",
      "INFO: Epoch 137: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.98\n",
      "INFO: Epoch 138: loss 1.497 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.691 | clip 0\n",
      "INFO: Epoch 138: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.98\n",
      "INFO: Epoch 139: loss 1.491 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.704 | clip 0\n",
      "INFO: Epoch 139: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.97\n",
      "INFO: Epoch 140: loss 1.494 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.648 | clip 0\n",
      "INFO: Epoch 140: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.97\n",
      "INFO: Epoch 141: loss 1.488 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.645 | clip 0\n",
      "INFO: Epoch 141: valid_loss 2.08 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.97\n",
      "INFO: Epoch 142: loss 1.484 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.695 | clip 0\n",
      "INFO: Epoch 142: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.96\n",
      "INFO: Epoch 143: loss 1.484 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.683 | clip 0\n",
      "INFO: Epoch 143: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.95\n",
      "INFO: Epoch 144: loss 1.484 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.701 | clip 0\n",
      "INFO: Epoch 144: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.95\n",
      "INFO: Epoch 145: loss 1.477 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.733 | clip 0.125\n",
      "INFO: Epoch 145: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.95\n",
      "INFO: Epoch 146: loss 1.478 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.623 | clip 0\n",
      "INFO: Epoch 146: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.94\n",
      "INFO: Epoch 147: loss 1.475 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.643 | clip 0\n",
      "INFO: Epoch 147: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.94\n",
      "INFO: Epoch 148: loss 1.469 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.735 | clip 0.125\n",
      "INFO: Epoch 148: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.94\n",
      "INFO: Epoch 149: loss 1.468 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.636 | clip 0\n",
      "INFO: Epoch 149: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.93\n",
      "INFO: Epoch 150: loss 1.467 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.653 | clip 0\n",
      "INFO: Epoch 150: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.94\n",
      "INFO: Epoch 151: loss 1.465 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.672 | clip 0\n",
      "INFO: Epoch 151: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.93\n",
      "INFO: Epoch 152: loss 1.466 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.639 | clip 0\n",
      "INFO: Epoch 152: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.92\n",
      "INFO: Epoch 153: loss 1.458 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.63 | clip 0\n",
      "INFO: Epoch 153: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.92\n",
      "INFO: Epoch 154: loss 1.457 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.63 | clip 0\n",
      "INFO: Epoch 154: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.92\n",
      "INFO: Epoch 155: loss 1.455 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.686 | clip 0\n",
      "INFO: Epoch 155: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.91\n",
      "INFO: Epoch 156: loss 1.449 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.615 | clip 0\n",
      "INFO: Epoch 156: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.9\n",
      "INFO: Epoch 157: loss 1.452 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.655 | clip 0\n",
      "INFO: Epoch 157: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.9\n",
      "INFO: Epoch 158: loss 1.45 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.631 | clip 0\n",
      "INFO: Epoch 158: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.9\n",
      "INFO: Epoch 159: loss 1.449 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.696 | clip 0\n",
      "INFO: Epoch 159: valid_loss 2.07 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.9\n",
      "INFO: Epoch 160: loss 1.449 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.67 | clip 0\n",
      "INFO: Epoch 160: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.88\n",
      "INFO: Epoch 161: loss 1.436 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.656 | clip 0\n",
      "INFO: Epoch 161: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.87\n",
      "INFO: Epoch 162: loss 1.439 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.659 | clip 0\n",
      "INFO: Epoch 162: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.88\n",
      "INFO: Epoch 163: loss 1.44 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.666 | clip 0\n",
      "INFO: Epoch 163: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.88\n",
      "INFO: Epoch 164: loss 1.438 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.69 | clip 0\n",
      "INFO: Epoch 164: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.87\n",
      "INFO: Epoch 165: loss 1.427 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.621 | clip 0\n",
      "INFO: Epoch 165: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.86\n",
      "INFO: Epoch 166: loss 1.431 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.648 | clip 0\n",
      "INFO: Epoch 166: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.86\n",
      "INFO: Epoch 167: loss 1.428 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.618 | clip 0\n",
      "INFO: Epoch 167: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.86\n",
      "INFO: Epoch 168: loss 1.423 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.646 | clip 0\n",
      "INFO: Epoch 168: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.85\n",
      "INFO: Epoch 169: loss 1.422 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.666 | clip 0\n",
      "INFO: Epoch 169: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.86\n",
      "INFO: Epoch 170: loss 1.426 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.692 | clip 0\n",
      "INFO: Epoch 170: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.85\n",
      "INFO: Epoch 171: loss 1.42 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.643 | clip 0\n",
      "INFO: Epoch 171: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.85\n",
      "INFO: Epoch 172: loss 1.415 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.668 | clip 0\n",
      "INFO: Epoch 172: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.85\n",
      "INFO: Epoch 173: loss 1.417 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.64 | clip 0\n",
      "INFO: Epoch 173: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.83\n",
      "INFO: Epoch 174: loss 1.411 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.626 | clip 0\n",
      "INFO: Epoch 174: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.83\n",
      "INFO: Epoch 175: loss 1.41 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.612 | clip 0\n",
      "INFO: Epoch 175: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.83\n",
      "INFO: Epoch 176: loss 1.41 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.639 | clip 0\n",
      "INFO: Epoch 176: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.83\n",
      "INFO: Epoch 177: loss 1.409 | lr 0.0003 | num_tokens 9.271 | batch_size 1250 | grad_norm 1.638 | clip 0\n",
      "INFO: Epoch 177: valid_loss 2.06 | num_tokens 9.14 | batch_size 500 | valid_perplexity 7.83\n",
      "INFO: No validation set improvements observed for 3 epochs. Early stop!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run from here on GPU"
   ],
   "metadata": {
    "id": "w1v7oZeWDBH5",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Translate our test set from French to English"
   ],
   "metadata": {
    "id": "acTJRczjEOWn",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python3 translate.py \\\n",
    "    --data /content/drive/MyDrive/atmt_2023_batch/data/en-fr/prepared \\\n",
    "    --dicts /content/drive/MyDrive/atmt_2023_batch/data/en-fr/prepared \\\n",
    "    --checkpoint-path /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/checkpoints/checkpoint_last.pt \\\n",
    "    --output /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/translations.p2.txt \\\n",
    "    --batch-size 128 \\\n",
    "    --cuda"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoCPauKHigp2",
    "outputId": "e21a5ac4-bf94-4d77-cbcf-31dd905d34c7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2023-11-06 15:56:15] COMMAND: translate.py --data /content/drive/MyDrive/atmt_2023_batch/data/en-fr/prepared --dicts /content/drive/MyDrive/atmt_2023_batch/data/en-fr/prepared --checkpoint-path /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/checkpoints/checkpoint_last.pt --output /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/translations.p2.txt --batch-size 128 --cuda\n",
      "[2023-11-06 15:56:15] Arguments: {'cuda': True, 'data': '/content/drive/MyDrive/atmt_2023_batch/data/en-fr/prepared', 'source_lang': 'fr', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 128, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': None, 'save_dir': '/content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0, 'seed': 42, 'dicts': '/content/drive/MyDrive/atmt_2023_batch/data/en-fr/prepared', 'checkpoint_path': '/content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/checkpoints/checkpoint_last.pt', 'output': '/content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/translations.p2.txt', 'max_len': 128}\n",
      "[2023-11-06 15:56:15] Loaded a source dictionary (fr) with 4000 words\n",
      "[2023-11-06 15:56:15] Loaded a target dictionary (en) with 4000 words\n",
      "[2023-11-06 15:56:15] Loaded a model from checkpoint /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/checkpoints/checkpoint_last.pt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To postprocess the generated translations and produce raw text, use the postprocess.sh located in the scripts directory"
   ],
   "metadata": {
    "id": "SZepKvydEY0T",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!bash /content/drive/MyDrive/atmt_2023_batch/scripts/postprocess.sh \\\n",
    "    /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/translations.p2.txt \\\n",
    "    /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/translations.p2.p.txt en"
   ],
   "metadata": {
    "id": "6HIZcoP0DFsy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To evaluate our NMT system, execute the following command:"
   ],
   "metadata": {
    "id": "0axKlIl1EnCB",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!cat \\\n",
    "    /content/drive/MyDrive/atmt_2023_batch/assignments/03/baseline/translations.p2.p.txt \\\n",
    "    | sacrebleu /content/drive/MyDrive/atmt_2023_batch/data/en-fr/raw/test.en"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KqfsJTppE0Qr",
    "outputId": "77456726-f2f1-4231-9446-a8deaadb35ed",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 26.2,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\",\n",
      " \"verbose_score\": \"59.0/32.6/20.1/12.5 (BP = 0.993 ratio = 0.993 hyp_len = 3864 ref_len = 3892)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.3.1\"\n",
      "}\n",
      "\u001B[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "FmLMZbf0UpvM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}